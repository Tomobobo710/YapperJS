{
  "help": { "type": "boolean", "short": "-h", "section": "info", "default": false, "description": "print usage and exit" },
  "version": { "type": "boolean", "section": "info", "default": false, "description": "show version and build info" },
  "cache-list": { "type": "boolean", "short": "-cl", "section": "info", "default": false, "description": "show list of models in cache" },
  "completion-bash": { "type": "boolean", "section": "info", "default": false, "description": "print source-able bash completion script" },
  "verbose-prompt": { "type": "boolean", "section": "info", "default": false, "description": "print a verbose prompt before generation" },
  "threads": { "type": "number", "short": "-t", "section": "cpu", "default": -1, "description": "number of CPU threads to use during generation" },
  "threads-batch": { "type": "number", "short": "-tb", "section": "cpu", "default": -1, "description": "number of threads to use during batch and prompt processing" },
  "cpu-mask": { "type": "text", "short": "-C", "section": "cpu", "default": "", "description": "CPU affinity mask: arbitrarily long hex" },
  "cpu-range": { "type": "text", "short": "-Cr", "section": "cpu", "default": null, "description": "range of CPUs for affinity (e.g., 0-3)" },
  "cpu-strict": { "type": "number", "options": ["0", "1"], "section": "cpu", "default": null, "description": "use strict CPU placement" },
  "prio": { "type": "number", "section": "cpu", "default": 0, "description": "set process/thread priority: -1=low, 0=normal, 1=medium, 2=high, 3=realtime" },
  "poll": { "type": "number", "section": "cpu", "default": 50, "description": "use polling level to wait for work (0-100)" },
  "cpu-mask-batch": { "type": "text", "short": "-Cb", "section": "cpu", "default": null, "description": "CPU affinity mask for batch processing" },
  "cpu-range-batch": { "type": "text", "short": "-Crb", "section": "cpu", "default": null, "description": "ranges of CPUs for affinity for batch" },
  "cpu-strict-batch": { "type": "number", "options": ["0", "1"], "section": "cpu", "default": null, "description": "use strict CPU placement for batch" },
  "prio-batch": { "type": "number", "section": "cpu", "default": 0, "description": "set process/thread priority for batch: 0=normal, 1=medium, 2=high, 3=realtime" },
  "poll-batch": { "type": "number", "options": ["0", "1"], "section": "cpu", "default": 0, "description": "use polling to wait for work for batch" },
  "ctx-size": { "type": "number", "short": "-c", "section": "essential", "default": 4096, "description": "size of the prompt context (0 = loaded from model)" },
  "n-predict": { "type": "number", "short": "-n", "section": "generation", "default": -1, "description": "number of tokens to predict (-1 = infinity)" },
  "batch-size": { "type": "number", "short": "-b", "section": "performance", "default": 2048, "description": "logical maximum batch size" },
  "ubatch-size": { "type": "number", "short": "-ub", "section": "performance", "default": 512, "description": "physical maximum batch size" },
  "keep": { "type": "number", "section": "generation", "default": 0, "description": "number of tokens to keep from initial prompt (-1 = all)" },
  "swa-full": { "type": "boolean", "section": "cache", "default": false, "description": "use full-size SWA cache" },
  "kv-unified": { "type": "boolean", "section": "cache", "default": false, "description": "use single unified KV buffer for all sequences" },
  "flash-attn": { "type": "select", "options": ["on", "off", "auto"], "short": "-fa", "section": "attention", "default": "auto", "description": "set Flash Attention use" },
  "no-perf": { "type": "boolean", "section": "performance", "default": false, "description": "disable internal libllama performance timings" },
  "escape": { "type": "boolean", "short": "-e", "section": "generation", "default": true, "description": "process escape sequences (\\n, \\r, \\t, \\', \\\", \\\\)" },
  "no-escape": { "type": "boolean", "section": "generation", "default": false, "description": "do not process escape sequences" },
  "rope-scaling": { "type": "select", "options": ["none", "linear", "yarn"], "section": "rope", "default": "linear", "description": "RoPE frequency scaling method" },
  "rope-scale": { "type": "number", "section": "rope", "default": 1.0, "description": "RoPE context scaling factor, expands context by a factor of N" },
  "rope-freq-base": { "type": "number", "section": "rope", "default": null, "description": "RoPE base frequency, used by NTK-aware scaling" },
  "rope-freq-scale": { "type": "number", "section": "rope", "default": 1.0, "description": "RoPE frequency scaling factor, expands context by a factor of 1/N" },
  "yarn-orig-ctx": { "type": "number", "section": "rope", "default": 0, "description": "YaRN: original context size of model (0 = model training context size)" },
  "yarn-ext-factor": { "type": "number", "section": "rope", "default": -1.0, "description": "YaRN: extrapolation mix factor (-1.0 = full interpolation)" },
  "yarn-attn-factor": { "type": "number", "section": "rope", "default": -1.0, "description": "YaRN: scale sqrt(t) or attention magnitude" },
  "yarn-beta-slow": { "type": "number", "section": "rope", "default": -1.0, "description": "YaRN: high correction dim or alpha" },
  "yarn-beta-fast": { "type": "number", "section": "rope", "default": -1.0, "description": "YaRN: low correction dim or beta" },
  "no-kv-offload": { "type": "boolean", "short": "-nkvo", "section": "cache", "default": false, "description": "disable KV offload" },
  "no-repack": { "type": "boolean", "short": "-nr", "section": "performance", "default": false, "description": "disable weight repacking" },
  "no-host": { "type": "boolean", "section": "memory", "default": false, "description": "bypass host buffer allowing extra buffers to be used" },
  "cache-type-k": { "type": "select", "options": ["f32", "f16", "bf16", "q8_0", "q4_0", "q4_1", "iq4_nl", "q5_0", "q5_1"], "short": "-ctk", "section": "cache", "default": "f16", "description": "KV cache data type for K" },
  "cache-type-v": { "type": "select", "options": ["f32", "f16", "bf16", "q8_0", "q4_0", "q4_1", "iq4_nl", "q5_0", "q5_1"], "short": "-ctv", "section": "cache", "default": "f16", "description": "KV cache data type for V" },
  "defrag-thold": { "type": "number", "short": "-dt", "section": "cache", "default": 0, "description": "KV cache defragmentation threshold (DEPRECATED)" },
  "parallel": { "type": "number", "short": "-np", "section": "performance", "default": 1, "description": "number of parallel sequences to decode" },
  "rpc": { "type": "text", "section": "network", "default": null, "description": "comma separated list of RPC servers" },
  "mlock": { "type": "boolean", "section": "memory", "default": false, "description": "force system to keep model in RAM rather than swapping" },
  "no-mmap": { "type": "boolean", "section": "memory", "default": false, "description": "do not memory-map model (slower load but may reduce pageouts)" },
  "numa": { "type": "select", "options": ["distribute", "isolate", "numactl"], "section": "cpu", "default": null, "description": "attempt optimizations that help on some NUMA systems" },
  "device": { "type": "text", "short": "-dev", "section": "gpu", "default": null, "description": "comma-separated list of devices to use for offloading" },
  "list-devices": { "type": "boolean", "section": "gpu", "default": false, "description": "print list of available devices and exit" },
  "override-tensor": { "type": "text", "short": "-ot", "section": "gpu", "default": null, "description": "override tensor buffer type using regex (e.g., -ot \".ffn_.*_exps.=CPU\" offloads all MoE layers to CPU)." },
  "cpu-moe": { "type": "boolean", "short": "-cmoe", "section": "gpu", "default": false, "description": "keep all Mixture of Experts (MoE) weights in the CPU" },
  "n-cpu-moe": { "type": "number", "short": "-ncmoe", "section": "gpu", "default": null, "description": "keep the Mixture of Experts (MoE) weights of the first N layers in the CPU" },
  "gpu-layers": { "type": "number", "short": "-ngl", "section": "gpu", "default": -1, "description": "max. number of layers to store in VRAM (-1 = all)" },
  "split-mode": { "type": "select", "options": ["none", "layer", "row"], "short": "-sm", "section": "gpu", "default": "layer", "description": "how to split the model across multiple GPUs" },
  "tensor-split": { "type": "text", "short": "-ts", "section": "gpu", "default": null, "description": "fraction of the model to offload to each GPU (e.g., 3,1 for 75% GPU0, 25% GPU1)" },
  "main-gpu": { "type": "number", "short": "-mg", "section": "gpu", "default": 0, "description": "the GPU to use for the model or for intermediate results" },
  "check-tensors": { "type": "boolean", "section": "gpu", "default": false, "description": "check model tensor data for invalid values" },
  "override-kv": { "type": "text", "section": "advanced", "default": null, "description": "advanced option to override model metadata (format: KEY=TYPE:VALUE)" },
  "no-op-offload": { "type": "boolean", "section": "gpu", "default": false, "description": "disable offloading host tensor operations to device" },
  "lora": { "type": "file", "section": "adapters", "default": null, "description": "path to LoRA adapter (can be repeated)" },
  "lora-scaled": { "type": "text", "section": "adapters", "default": null, "description": "path to LoRA adapter with user defined scaling" },
  "control-vector": { "type": "file", "section": "adapters", "default": null, "description": "add a control vector" },
  "control-vector-scaled": { "type": "text", "section": "adapters", "default": null, "description": "add a control vector with user defined scaling" },
  "control-vector-layer-range": { "type": "text", "section": "adapters", "default": null, "description": "layer range to apply the control vector(s) to (START END)" },
  "model": { "type": "file", "short": "-m", "section": "essential", "default": null, "description": "model path to load (not required if a preset is enabled)" },
  "model-url": { "type": "text", "short": "-mu", "section": "model-sources", "default": null, "description": "model download url" },
  "docker-repo": { "type": "text", "short": "-dr", "section": "model-sources", "default": null, "description": "Docker Hub model repository (format: [repo/]model[:quant])" },
  "hf-repo": { "type": "text", "short": "-hf", "section": "model-sources", "default": null, "description": "Hugging Face model repository (format: user/model[:quant])" },
  "hf-repo-draft": { "type": "text", "short": "-hfd", "section": "speculative", "default": null, "description": "Hugging Face model repository for draft model" },
  "hf-file": { "type": "text", "short": "-hff", "section": "model-sources", "default": null, "description": "Hugging Face model file" },
  "hf-repo-v": { "type": "text", "short": "-hfv", "section": "model-sources", "default": null, "description": "Hugging Face model repository for vocoder model" },
  "hf-file-v": { "type": "text", "short": "-hffv", "section": "model-sources", "default": null, "description": "Hugging Face model file for vocoder model" },
  "hf-token": { "type": "text", "short": "-hft", "section": "model-sources", "default": null, "description": "Hugging Face access token" },
  "log-disable": { "type": "boolean", "section": "logging", "default": false, "description": "Log disable" },
  "log-file": { "type": "file", "section": "logging", "default": null, "description": "Log to file" },
  "log-colors": { "type": "select", "options": ["on", "off", "auto"], "section": "logging", "default": "auto", "description": "Set colored logging" },
  "verbose": { "type": "boolean", "short": "-v", "section": "logging", "default": false, "description": "Set verbosity level to infinity (log all messages)" },
  "offline": { "type": "boolean", "section": "network", "default": false, "description": "Offline mode: forces use of cache, prevents network access" },
  "log-verbosity": { "type": "number", "short": "-lv", "section": "logging", "default": 3, "description": "Set the verbosity threshold (0=generic, 1=error, 2=warning, 3=info, 4=debug)" },
  "log-prefix": { "type": "boolean", "section": "logging", "default": false, "description": "Enable prefix in log messages" },
  "log-timestamps": { "type": "boolean", "section": "logging", "default": false, "description": "Enable timestamps in log messages" },
  "cache-type-k-draft": { "type": "select", "options": ["f32", "f16", "bf16", "q8_0", "q4_0", "q4_1", "iq4_nl", "q5_0", "q5_1"], "section": "speculative", "default": "f16", "description": "KV cache data type for K for draft model" },
  "cache-type-v-draft": { "type": "select", "options": ["f32", "f16", "bf16", "q8_0", "q4_0", "q4_1", "iq4_nl", "q5_0", "q5_1"], "section": "speculative", "default": "f16", "description": "KV cache data type for V for draft model" },
  "samplers": { "type": "text", "section": "sampling", "default": "penalties;dry;top_n_sigma;top_k;typ_p;top_p;min_p;xtc;temperature", "description": "samplers that will be used for generation in order" },
  "seed": { "type": "number", "short": "-s", "section": "generation", "default": -1, "description": "RNG seed (-1 = random)" },
  "sampling-seq": { "type": "text", "section": "sampling", "default": "edskypmxt", "description": "simplified sequence for samplers" },
  "ignore-eos": { "type": "boolean", "section": "generation", "default": false, "description": "ignore end of stream token and continue generating" },
  "temp": { "type": "number", "section": "sampling", "default": 0.8, "description": "temperature" },
  "top-k": { "type": "number", "section": "sampling", "default": 40, "description": "top-k sampling (0 = disabled)" },
  "top-p": { "type": "number", "section": "sampling", "default": 0.95, "description": "top-p sampling (1.0 = disabled)" },
  "min-p": { "type": "number", "section": "sampling", "default": 0.05, "description": "min-p sampling (0.0 = disabled)" },
  "top-nsigma": { "type": "number", "section": "sampling", "default": -1.0, "description": "top-n-sigma sampling (-1.0 = disabled)" },
  "xtc-probability": { "type": "number", "section": "sampling", "default": 0.0, "description": "xtc probability (0.0 = disabled)" },
  "xtc-threshold": { "type": "number", "section": "sampling", "default": 0.1, "description": "xtc threshold (1.0 = disabled)" },
  "typical": { "type": "number", "section": "sampling", "default": 1.0, "description": "locally typical sampling parameter (1.0 = disabled)" },
  "repeat-last-n": { "type": "number", "section": "sampling", "default": 64, "description": "last n tokens to consider for penalize (-1 = ctx_size)" },
  "repeat-penalty": { "type": "number", "section": "sampling", "default": 1.0, "description": "penalize repeat sequence of tokens (1.0 = disabled)" },
  "presence-penalty": { "type": "number", "section": "sampling", "default": 0.0, "description": "repeat alpha presence penalty (0.0 = disabled)" },
  "frequency-penalty": { "type": "number", "section": "sampling", "default": 0.0, "description": "repeat alpha frequency penalty (0.0 = disabled)" },
  "dry-multiplier": { "type": "number", "section": "sampling", "default": 0.0, "description": "set DRY sampling multiplier (0.0 = disabled)" },
  "dry-base": { "type": "number", "section": "sampling", "default": 1.75, "description": "set DRY sampling base value" },
  "dry-allowed-length": { "type": "number", "section": "sampling", "default": 2, "description": "set allowed length for DRY sampling" },
  "dry-penalty-last-n": { "type": "number", "section": "sampling", "default": -1, "description": "set DRY penalty for the last n tokens (-1 = context size)" },
  "dry-sequence-breaker": { "type": "text", "section": "sampling", "default": null, "description": "add sequence breaker for DRY sampling" },
  "dynatemp-range": { "type": "number", "section": "sampling", "default": 0.0, "description": "dynamic temperature range (0.0 = disabled)" },
  "dynatemp-exp": { "type": "number", "section": "sampling", "default": 1.0, "description": "dynamic temperature exponent" },
  "mirostat": { "type": "number", "section": "sampling", "default": 0, "description": "use Mirostat sampling (0=disabled, 1=Mirostat, 2=Mirostat 2.0)" },
  "mirostat-lr": { "type": "number", "section": "sampling", "default": 0.1, "description": "Mirostat learning rate (eta)" },
  "mirostat-ent": { "type": "number", "section": "sampling", "default": 5.0, "description": "Mirostat target entropy (tau)" },
  "logit-bias": { "type": "text", "short": "-l", "section": "sampling", "default": null, "description": "modifies the likelihood of token (format: TOKEN_ID+/-BIAS)" },
  "grammar": { "type": "text", "section": "constraints", "default": null, "description": "BNF-like grammar to constrain generations" },
  "grammar-file": { "type": "file", "section": "constraints", "default": null, "description": "file to read grammar from" },
  "json-schema": { "type": "text", "short": "-j", "section": "constraints", "default": null, "description": "JSON schema to constrain generations" },
  "json-schema-file": { "type": "file", "short": "-jf", "section": "constraints", "default": null, "description": "File containing a JSON schema" },
  "ctx-checkpoints": { "type": "number", "section": "cache", "default": 8, "description": "max number of context checkpoints to create per slot" },
  "cache-ram": { "type": "number", "short": "-cram", "section": "cache", "default": 8192, "description": "set the maximum cache size in MiB (-1 = no limit, 0 = disable)" },
  "no-context-shift": { "type": "boolean", "section": "generation", "default": false, "description": "disables context shift on infinite text generation" },
  "context-shift": { "type": "boolean", "section": "generation", "default": false, "description": "enables context shift on infinite text generation" },
  "reverse-prompt": { "type": "text", "short": "-r", "section": "generation", "default": null, "description": "halt generation at PROMPT, return control in interactive mode" },
  "special": { "type": "boolean", "short": "-sp", "section": "generation", "default": false, "description": "special tokens output enabled" },
  "no-warmup": { "type": "boolean", "section": "performance", "default": false, "description": "skip warming up the model with an empty run" },
  "spm-infill": { "type": "boolean", "section": "embeddings", "default": false, "description": "use Suffix/Prefix/Middle pattern for infill" },
  "pooling": { "type": "select", "options": ["none", "mean", "cls", "last", "rank"], "section": "embeddings", "default": null, "description": "pooling type for embeddings" },
  "cont-batching": { "type": "boolean", "short": "-cb", "section": "performance", "default": true, "description": "enable continuous batching (dynamic batching)" },
  "no-cont-batching": { "type": "boolean", "short": "-nocb", "section": "performance", "default": false, "description": "disable continuous batching" },
  "mmproj": { "type": "file", "section": "multimodal", "default": null, "description": "path to a multimodal projector file" },
  "mmproj-url": { "type": "text", "section": "multimodal", "default": null, "description": "URL to a multimodal projector file" },
  "no-mmproj": { "type": "boolean", "section": "multimodal", "default": false, "description": "explicitly disable multimodal projector" },
  "no-mmproj-offload": { "type": "boolean", "section": "multimodal", "default": false, "description": "do not offload multimodal projector to GPU" },
  "image-min-tokens": { "type": "number", "section": "multimodal", "default": null, "description": "minimum number of tokens each image can take" },
  "image-max-tokens": { "type": "number", "section": "multimodal", "default": null, "description": "maximum number of tokens each image can take" },
  "override-tensor-draft": { "type": "text", "section": "speculative", "default": null, "description": "override tensor buffer type for draft model" },
  "cpu-moe-draft": { "type": "boolean", "short": "-cmoed", "section": "speculative", "default": false, "description": "keep all Mixture of Experts (MoE) weights in the CPU for draft model" },
  "n-cpu-moe-draft": { "type": "number", "short": "-ncmoed", "section": "speculative", "default": null, "description": "keep the Mixture of Experts (MoE) weights of the first N layers in the CPU for draft model" },
  "alias": { "type": "text", "short": "-a", "section": "api", "default": null, "description": "set alias for model name (to be used by REST API)" },
  "host": { "type": "text", "section": "server", "default": "127.0.0.1", "description": "ip address to listen, or bind to UNIX socket if address ends with .sock" },
  "port": { "type": "number", "section": "server", "default": 8080, "description": "port to listen" },
  "path": { "type": "file", "section": "server", "default": "", "description": "path to serve static files from" },
  "api-prefix": { "type": "text", "section": "api", "default": null, "description": "prefix path the server serves from, without the trailing slash" },
  "no-webui": { "type": "boolean", "section": "features", "default": false, "description": "Disable the Web UI" },
  "embedding": { "type": "boolean", "section": "features", "default": false, "description": "restrict to only support embedding use case" },
  "reranking": { "type": "boolean", "section": "features", "default": false, "description": "enable reranking endpoint on server" },
  "api-key": { "type": "text", "section": "api", "default": null, "description": "API key to use for authentication" },
  "api-key-file": { "type": "file", "section": "api", "default": null, "description": "path to file containing API keys" },
  "ssl-key-file": { "type": "file", "section": "security", "default": null, "description": "path to file a PEM-encoded SSL private key" },
  "ssl-cert-file": { "type": "file", "section": "security", "default": null, "description": "path to file a PEM-encoded SSL certificate" },
  "chat-template-kwargs": { "type": "text", "section": "templates", "default": null, "description": "sets additional params for the json template parser" },
  "timeout": { "type": "number", "short": "-to", "section": "server", "default": 600, "description": "server read/write timeout in seconds" },
  "threads-http": { "type": "number", "section": "server", "default": null, "description": "number of threads used to process HTTP requests" },
  "cache-reuse": { "type": "number", "section": "cache", "default": 0, "description": "min chunk size to attempt reusing from the cache via KV shifting" },
  "metrics": { "type": "boolean", "section": "api", "default": false, "description": "enable prometheus compatible metrics endpoint" },
  "props": { "type": "boolean", "section": "api", "default": false, "description": "enable changing global properties via POST /props" },
  "slots": { "type": "boolean", "section": "api", "default": true, "description": "enable slots monitoring endpoint" },
  "no-slots": { "type": "boolean", "section": "api", "default": false, "description": "disables slots monitoring endpoint" },
  "slot-save-path": { "type": "file", "section": "api", "default": null, "description": "path to save slot kv cache" },
  "models-dir": { "type": "file", "section": "routing", "default": null, "description": "directory containing models for the router server" },
  "models-max": { "type": "number", "section": "routing", "default": 4, "description": "for router server, maximum number of models to load simultaneously (0 = unlimited)" },
  "no-models-autoload": { "type": "boolean", "section": "routing", "default": false, "description": "disables automatic loading of models" },
  "jinja": { "type": "boolean", "section": "templates", "default": true, "description": "use jinja template for chat" },
  "no-jinja": { "type": "boolean", "section": "templates", "default": false, "description": "disable jinja template for chat" },
  "reasoning-format": { "type": "select", "options": ["none", "deepseek", "deepseek-legacy"], "section": "reasoning", "default": "auto", "description": "controls whether thought tags are allowed/extracted from response" },
  "reasoning-budget": { "type": "number", "section": "reasoning", "default": -1, "description": "controls the amount of thinking allowed (-1 = unrestricted, 0 = disable)" },
  "chat-template": { "type": "text", "section": "templates", "default": null, "description": "set custom jinja chat template" },
  "chat-template-file": { "type": "file", "section": "templates", "default": null, "description": "set custom jinja chat template file" },
  "no-prefill-assistant": { "type": "boolean", "section": "templates", "default": false, "description": "whether to prefill the assistant's response if last message is assistant" },
  "slot-prompt-similarity": { "type": "number", "short": "-sps", "section": "api", "default": 0.1, "description": "how much the prompt must match to use that slot (0.0 = disabled)" },
  "threads-draft": { "type": "number", "short": "-td", "section": "speculative", "default": -1, "description": "number of threads to use during generation for draft model" },
  "threads-batch-draft": { "type": "number", "short": "-tbd", "section": "speculative", "default": -1, "description": "number of threads to use during batch processing for draft model" },
  "draft-max": { "type": "number", "section": "speculative", "default": 16, "description": "number of tokens to draft for speculative decoding" },
  "draft-min": { "type": "number", "section": "speculative", "default": 0, "description": "minimum number of draft tokens for speculative decoding" },
  "draft-p-min": { "type": "number", "section": "speculative", "default": 0.8, "description": "minimum speculative decoding probability (greedy)" },
  "ctx-size-draft": { "type": "number", "short": "-cd", "section": "speculative", "default": 0, "description": "size of the prompt context for draft model (0 = loaded from model)" },
  "device-draft": { "type": "text", "short": "-devd", "section": "speculative", "default": null, "description": "comma-separated list of devices for offloading draft model" },
  "gpu-layers-draft": { "type": "number", "short": "-ngld", "section": "speculative", "default": null, "description": "number of layers to store in VRAM for draft model" },
  "model-draft": { "type": "file", "short": "-md", "section": "speculative", "default": null, "description": "draft model for speculative decoding" },
  "spec-replace": { "type": "text", "section": "speculative", "default": null, "description": "translate string in TARGET into DRAFT if models are not compatible" },
  "model-vocoder": { "type": "file", "section": "audio", "default": null, "description": "vocoder model for audio generation" },
  "tts-use-guide-tokens": { "type": "boolean", "section": "audio", "default": false, "description": "Use guide tokens to improve TTS word recall" },
  "embd-gemma-default": { "type": "boolean", "section": "presets", "default": false, "description": "use default EmbeddingGemma model" },
  "fim-qwen-1.5b-default": { "type": "boolean", "section": "presets", "default": false, "description": "use default Qwen 2.5 Coder 1.5B" },
  "fim-qwen-3b-default": { "type": "boolean", "section": "presets", "default": false, "description": "use default Qwen 2.5 Coder 3B" },
  "fim-qwen-7b-default": { "type": "boolean", "section": "presets", "default": false, "description": "use default Qwen 2.5 Coder 7B" },
  "fim-qwen-7b-spec": { "type": "boolean", "section": "presets", "default": false, "description": "use Qwen 2.5 Coder 7B + 0.5B draft for speculative decoding" },
  "fim-qwen-14b-spec": { "type": "boolean", "section": "presets", "default": false, "description": "use Qwen 2.5 Coder 14B + 0.5B draft for speculative decoding" },
  "fim-qwen-30b-default": { "type": "boolean", "section": "presets", "default": false, "description": "use default Qwen 3 Coder 30B A3B Instruct" },
  "gpt-oss-20b-default": { "type": "boolean", "section": "presets", "default": false, "description": "use gpt-oss-20b" },
  "gpt-oss-120b-default": { "type": "boolean", "section": "presets", "default": false, "description": "use gpt-oss-120b" },
  "vision-gemma-4b-default": { "type": "boolean", "section": "presets", "default": false, "description": "use Gemma 3 4B QAT" },
  "vision-gemma-12b-default": { "type": "boolean", "section": "presets", "default": false, "description": "use Gemma 3 12B QAT" }
}
